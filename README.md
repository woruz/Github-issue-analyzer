# ğŸš€ GitHub Issue Analyzer  
### _Local Caching + LLM Processing_

![Node.js](https://img.shields.io/badge/Node.js-18+-green)
![SQLite](https://img.shields.io/badge/SQLite-Local%20Storage-blue)
![LLM](https://img.shields.io/badge/LLM-Ollama-orange)
![Backend](https://img.shields.io/badge/Type-Backend%20Only-lightgrey)

---

## ğŸ“Œ Overview

**GitHub Issue Analyzer** is a backend service built with **Node.js** that enables intelligent analysis of GitHub issues using **Large Language Models (LLMs)**.

The service is designed to:

- ğŸ” **Fetch and locally cache open GitHub issues** from any repository  
- ğŸ§  **Analyze cached issues using a natural-language prompt** and an LLM  
- âš¡ Provide fast, reusable insights without repeatedly hitting the GitHub API  

This project is **backend-only** and exposes two REST APIs:  
**`/scan`** and **`/analyze`**

---

## ğŸ¯ Key Focus Areas

- Clean and intuitive **REST API design**
- **Local persistence** of GitHub issues
- **LLM-based reasoning** over real-world data
- Scalable, production-like **backend architecture**

> No UI is included â€” this project strictly focuses on backend engineering and intelligent data processing.

---

## ğŸ› ï¸ Tech Stack

- **Node.js**
- **Express.js** â€“ REST API server
- **Axios** â€“ GitHub API integration
- **SQLite** â€“ Local persistent storage
- **Ollama API** â€“ LLM-powered analysis
- **dotenv** â€“ Environment variable management

---

## ğŸ“‚ Project Structure
```
github-issue-analyzer/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ controller/
â”‚ â”‚ â”œâ”€â”€ analyze/
â”‚ â”‚ â”‚ â”œâ”€â”€ analyze.controller.js # Analyze API controller
â”‚ â”‚ â”‚ â””â”€â”€ analyze.helper.js # LLM (Ollama) helper logic
â”‚ â”‚ â””â”€â”€ github/
â”‚ â”‚ â”œâ”€â”€ github.controller.js # GitHub scan controller
â”‚ â”‚ â””â”€â”€ github.helper.js # GitHub API helper logic
â”‚ â”‚
â”‚ â”œâ”€â”€ platform/
â”‚ â”‚ â””â”€â”€ db.js # SQLite database setup
â”‚ â”‚
â”‚ â”œâ”€â”€ routes/
â”‚ â”‚ â””â”€â”€ routes.js # API route definitions
â”‚ â”‚
â”‚ â””â”€â”€ index.js # Application entry point
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ issues.db # SQLite database (generated at runtime)
â”‚
â”œâ”€â”€ .env # Environment variables (not committed)
â”œâ”€â”€ .env.sample # Sample environment configuration
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â”œâ”€â”€ package-lock.json
â””â”€â”€ README.md
```

### ğŸ“Œ Notes on Structure
```bash
- Controllers handle **request/response logic**
- Helpers encapsulate **GitHub API and LLM (Ollama) logic**
- `platform/db.js` is responsible for **database initialization**
- `issues.db` is generated at runtime and excluded from version control
```
---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/woruz/Github-issue-analyzer.git
cd Github-issue-analyzer
```

2ï¸âƒ£ Install dependencies
```bash
npm install
```

3ï¸âƒ£ Configure environment variables
```bash
Create a .env file in the root directory:

PORT=3000
GITHUB_TOKEN=your_github_personal_access_token
OLLAMA_API_URL=http://localhost:11434/api/generate
MODEL=your_model_name
```

â–¶ï¸ Running the Server
```bash
node src/index.js
```

ğŸ”Œ API Endpoints

ğŸ” POST /scan

Purpose:-
Fetch and locally cache all open GitHub issues for a given repository.

```json
Request Body
{
  "repo": "owner/repository-name"
}

Example:
{
  "repo": "facebook/react"
}


What Happens
Calls the GitHub REST API
Filters out pull requests
Extracts relevant issue data
Stores issues in local SQLite database

Response
{
  "repo": "facebook/react",
  "issues_fetched": 42,
  "cached_successfully": true
}
```



ğŸ§  POST /analyze
Purpose:-
Analyze cached GitHub issues using a natural-language prompt and an LLM.

```json
Request Body
{
  "repo": "facebook/react",
  "prompt": "Find common themes and recommend what should be fixed first"
}


What Happens
Retrieves cached issues for the repository
Combines issues with user prompt
Sends context to an LLM
Returns a natural-language analysis

Response
{
  "analysis": "Most issues relate to crashes and memory leaks. Maintainers should prioritize fixing startup crashes and lifecycle-related bugs..."
}
```

ğŸ’¾ Storage Choice & Reasoning
âœ… Chosen Storage: SQLite

### Why SQLite?

```bash
Persistent storage across server restarts
No external database required
Easy to inspect and query
Scales better than in-memory or JSON storage
Closely mirrors real-world backend systems
This makes SQLite the best balance between simplicity and production realism for this assignment.

```

### âš ï¸ Edge Case Handling

```bash
The service gracefully handles:
Repository not yet scanned
No cached issues available
GitHub API errors or rate limits
LLM API failures
Empty or invalid input
```

### ğŸ§ª Example Usage (cURL)

```bash
Scan a Repository
curl -X POST http://localhost:3000/scan \
  -H "Content-Type: application/json" \
  -d '{"repo":"facebook/react"}'

Analyze Issues
curl -X POST http://localhost:3000/analyze \
  -H "Content-Type: application/json" \
  -d '{"repo":"facebook/react","prompt":"Summarize major issue themes"}'
```

## ğŸ¤– Prompts used

During development, the following LLM-related queries and investigations were performed.

### Queries Regarding OpenAI Credentials

- â€œHow do I create OpenAI API credentials for using the Chat Completion API?â€
- â€œHow to generate and configure an OpenAI API key for a Node.js backend?â€

---

### Queries Regarding OpenAI API Issues

- â€œWhy is the OpenAI API returning authentication or quota errors?â€
- â€œWhy is the OpenAI API not working despite a valid API key?â€

These queries helped identify issues related to:
- API quota limitations
- Billing requirements

---

### Queries Exploring LLM Alternatives

- â€œWhat are good alternatives to OpenAI for LLM-based text analysis?â€
- â€œHow to use local LLMs instead of cloud-based APIs?â€
- â€œHow to integrate Ollama with a Node.js backend?â€
- â€œHow to run LLama or Mistral or phi3 locally using Ollama?â€
- â€œWhy is Mistral or LLama model not working in my system?â€
- â€œGive me a model that utilizes less RAM?â€
- â€œDifference between CPU and GPU?â€

---

### Final Decision

Based on these investigations, the project uses **Ollama** for local LLM inference.  
This approach removes external API dependencies, avoids rate limits and billing issues, and allows reliable natural-language analysis directly on the developerâ€™s machine.


### ğŸ‘¤ Author
```bash
Sabil Danish
```
